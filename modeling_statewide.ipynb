{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.layers import Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras import Sequential\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import callbacks\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPool2D\n",
    "from keras.layers import ReLU\n",
    "import numpy as np\n",
    "\n",
    "from ipynb.fs.full.models import make_vgg16, make_nn\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "import cv2\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "suff = '_8.29'\n",
    "cols = ['wpop_diff', 'bpop_diff', 'area_diff', 'perim_diff','label', 'image_file']\n",
    "over_cols = ['wpop_diff', 'bpop_diff', 'area_diff', 'image_file', 'label']\n",
    "\n",
    "demo_train_df = pd.read_csv(f'Outputs/statewide_train_data{suff}.csv', header=0, names=cols)\n",
    "demo_test_df = pd.read_csv(f'Outputs/statewide_test_data{suff}.csv', header=0, names=cols)\n",
    "demo_val_df = pd.read_csv(f'Outputs/statewide_val_data{suff}.csv', header=0, names=cols)\n",
    "\n",
    "over_demo_train_df = pd.read_csv('Outputs/statewide_train_data_oversample.csv', header=0, names=over_cols)\n",
    "over_demo_test_df = pd.read_csv('Outputs/statewide_test_data_oversample.csv', header=0, names=over_cols)\n",
    "over_demo_val_df = pd.read_csv('Outputs/statewide_val_data_oversample.csv', header=0, names=over_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_report(fit_model, test_data, test_labels, title):\n",
    "    \"\"\"\n",
    "    Generates confusion matrix and classification report for given model on provided test data.\n",
    "\n",
    "    Paramters: \n",
    "        fit_model: Keras.model - compiled Keras model\n",
    "        test_data: np.array, pd.Dataframe column, list - X input for test data\n",
    "        test_labels: np.array, pd.Dataframe column, list - y input for test data (labels)\n",
    "        title: str - title of confusion matrix\n",
    "\n",
    "    return: \n",
    "        None - generates the confusion matrix that can then be saved and prints the classification report \n",
    "    \"\"\"\n",
    "    # make model predictions on test data\n",
    "    preds = fit_model.predict(test_data)\n",
    "\n",
    "    # get Boolean label from continuous prediction confidence score\n",
    "    pred_labels = [False if x < 0.5 else True for x in preds]\n",
    "\n",
    "    # generate and show confusion matrix\n",
    "    conf = confusion_matrix(test_labels, pred_labels)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=conf)\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "    # print classification report\n",
    "    print(classification_report(test_labels, pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(df, cols):\n",
    "    \"\"\"\n",
    "    Min-max normalizes provided columns within dataframe.\n",
    "\n",
    "    Paramters: \n",
    "        df: pd.Dataframe - dataframe with columns to normalize\n",
    "        cols: list - list of column names as strings to normalize\n",
    "\n",
    "    return: \n",
    "        None - Normalizes the provided columns in df in place\n",
    "    \"\"\"\n",
    "    x = df[cols].values\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    x_scaled = min_max_scaler.fit_transform(x)\n",
    "    df[cols] = x_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_array(df_image_col, subset=None, resize=True):\n",
    "    \"\"\"\n",
    "    Converts images to np.array format for use in Keras models.\n",
    "\n",
    "    Paramters: \n",
    "        df_image_col: pd.Dataframe column, list - column in dataframe with image file names as strings\n",
    "        subset: str - subset that the data belongs to. One of three options: 'train', 'test', 'val'\n",
    "        resize: Boolean - whether the images have been preprocessed/resized. Default=False\n",
    "\n",
    "    return: \n",
    "        ret_arr: np.array - numerical representation of images in single array\n",
    "    \"\"\"\n",
    "    # initialize return array\n",
    "    ret_arr = []\n",
    "\n",
    "    # loop through image file names\n",
    "    for i in df_image_col:\n",
    "\n",
    "        # read and append image to return array\n",
    "        if resize:\n",
    "            im = cv2.imread(f'Outputs/images/statewide/{subset}/processed/' + i)\n",
    "        \n",
    "        else:\n",
    "            im = cv2.imread('Outputs/images/statewide/' + i)\n",
    "        \n",
    "        ret_arr.append(im)\n",
    "        \n",
    "    return np.array(ret_arr, dtype=object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize numerical features\n",
    "normal = True\n",
    "\n",
    "if normal:\n",
    "    df_list = [demo_test_df, demo_train_df, demo_val_df] #, over_demo_test_df, over_demo_train_df, over_demo_val_df]\n",
    "    norm_cols = ['wpop_diff', 'bpop_diff', 'area_diff', 'perim_diff']\n",
    "    for data in df_list:\n",
    "        normalize(data, norm_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wpop_diff</th>\n",
       "      <th>bpop_diff</th>\n",
       "      <th>area_diff</th>\n",
       "      <th>perim_diff</th>\n",
       "      <th>label</th>\n",
       "      <th>image_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>711</th>\n",
       "      <td>0.652815</td>\n",
       "      <td>0.611372</td>\n",
       "      <td>0.551100</td>\n",
       "      <td>0.396517</td>\n",
       "      <td>False</td>\n",
       "      <td>state_plot7120.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.283966</td>\n",
       "      <td>0.323942</td>\n",
       "      <td>0.554893</td>\n",
       "      <td>0.539535</td>\n",
       "      <td>False</td>\n",
       "      <td>state_plot990.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>937</th>\n",
       "      <td>0.418857</td>\n",
       "      <td>0.325763</td>\n",
       "      <td>0.623737</td>\n",
       "      <td>0.436361</td>\n",
       "      <td>True</td>\n",
       "      <td>state_plot9380.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357</th>\n",
       "      <td>0.505302</td>\n",
       "      <td>0.466818</td>\n",
       "      <td>0.725118</td>\n",
       "      <td>0.359819</td>\n",
       "      <td>False</td>\n",
       "      <td>state_plot3580.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>701</th>\n",
       "      <td>0.279588</td>\n",
       "      <td>0.372019</td>\n",
       "      <td>0.841260</td>\n",
       "      <td>0.178652</td>\n",
       "      <td>True</td>\n",
       "      <td>state_plot7020.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>700</th>\n",
       "      <td>0.422763</td>\n",
       "      <td>0.449401</td>\n",
       "      <td>0.842477</td>\n",
       "      <td>0.157667</td>\n",
       "      <td>False</td>\n",
       "      <td>state_plot7010.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>0.555319</td>\n",
       "      <td>0.616850</td>\n",
       "      <td>0.345580</td>\n",
       "      <td>0.110396</td>\n",
       "      <td>True</td>\n",
       "      <td>state_plot1790.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>0.284044</td>\n",
       "      <td>0.333282</td>\n",
       "      <td>0.412128</td>\n",
       "      <td>0.165183</td>\n",
       "      <td>True</td>\n",
       "      <td>state_plot3000.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408</th>\n",
       "      <td>0.205654</td>\n",
       "      <td>0.133490</td>\n",
       "      <td>0.301037</td>\n",
       "      <td>0.579995</td>\n",
       "      <td>True</td>\n",
       "      <td>state_plot4090.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>519</th>\n",
       "      <td>0.291637</td>\n",
       "      <td>0.278305</td>\n",
       "      <td>0.711729</td>\n",
       "      <td>0.397601</td>\n",
       "      <td>False</td>\n",
       "      <td>state_plot5200.png</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>720 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     wpop_diff  bpop_diff  area_diff  perim_diff  label          image_file\n",
       "711   0.652815   0.611372   0.551100    0.396517  False  state_plot7120.png\n",
       "98    0.283966   0.323942   0.554893    0.539535  False   state_plot990.png\n",
       "937   0.418857   0.325763   0.623737    0.436361   True  state_plot9380.png\n",
       "357   0.505302   0.466818   0.725118    0.359819  False  state_plot3580.png\n",
       "701   0.279588   0.372019   0.841260    0.178652   True  state_plot7020.png\n",
       "..         ...        ...        ...         ...    ...                 ...\n",
       "700   0.422763   0.449401   0.842477    0.157667  False  state_plot7010.png\n",
       "178   0.555319   0.616850   0.345580    0.110396   True  state_plot1790.png\n",
       "299   0.284044   0.333282   0.412128    0.165183   True  state_plot3000.png\n",
       "408   0.205654   0.133490   0.301037    0.579995   True  state_plot4090.png\n",
       "519   0.291637   0.278305   0.711729    0.397601  False  state_plot5200.png\n",
       "\n",
       "[720 rows x 6 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create image arrays for train/test/val sets\n",
    "\n",
    "# normal, unbalanced data\n",
    "train_images = img_array(demo_train_df['image_file'], resize=False)\n",
    "test_images = img_array(demo_test_df['image_file'], resize=False)\n",
    "val_images = img_array(demo_val_df['image_file'], resize=False)\n",
    "\n",
    "# oversampled data\n",
    "over_train_images = img_array(over_demo_train_df['image_file'], resize=False)\n",
    "over_test_images = img_array(over_demo_test_df['image_file'], resize=False)\n",
    "over_val_images = img_array(over_demo_val_df['image_file'], resize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make individual CNN and NN models with functions from models.ipynb\n",
    "cnn = make_vgg16()\n",
    "nn = make_nn(4)\n",
    "\n",
    "# combine the model outputs\n",
    "combined_model = tf.keras.layers.concatenate([nn.output, cnn.output],axis=1)\n",
    "\n",
    "# final output layers\n",
    "x = Dense(4, activation='relu')(combined_model)\n",
    "x = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inputs=[nn.input, cnn.input], outputs=x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Adam(learning_rate=1e-3, epsilon=1e-3 / 200)\n",
    "model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit, Train, and Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "early = EarlyStopping(monitor='val_accuracy', min_delta=0, patience=3, verbose=1, mode='auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Unbalanced Data Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "23/23 [==============================] - 125s 5s/step - loss: 0.6935 - accuracy: 0.6194 - val_loss: 0.6901 - val_accuracy: 0.6000\n",
      "Epoch 2/20\n",
      "23/23 [==============================] - 126s 6s/step - loss: 0.6870 - accuracy: 0.6333 - val_loss: 0.6863 - val_accuracy: 0.6000\n",
      "Epoch 3/20\n",
      "23/23 [==============================] - 128s 6s/step - loss: 0.6780 - accuracy: 0.6333 - val_loss: 0.6802 - val_accuracy: 0.6000\n",
      "Epoch 4/20\n",
      "23/23 [==============================] - 125s 5s/step - loss: 0.6698 - accuracy: 0.6333 - val_loss: 0.6776 - val_accuracy: 0.6000\n",
      "Epoch 00004: early stopping\n"
     ]
    }
   ],
   "source": [
    "statewide_model = model.fit(\n",
    "\tx=[demo_train_df.iloc[:,:-2].values.astype(np.float32), train_images.astype(np.float32)], \n",
    "\ty=demo_train_df['label'].values,\n",
    "\tvalidation_data=([demo_val_df.iloc[:,:-2].values.astype(np.float32), val_images.astype(np.float32)], demo_val_df['label'].values),\n",
    "\tcallbacks=early,\n",
    "\tepochs=20,\n",
    "\tbatch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 29s 4s/step - loss: 0.6823 - accuracy: 0.5800\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6822792291641235, 0.5799999833106995]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate model\n",
    "eval_results = model.evaluate(x=[demo_test_df.iloc[:,:-2].values.astype(np.float32), test_images.astype(np.float32)],\n",
    "y=demo_test_df['label'].values)\n",
    "\n",
    "eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.68      1.00      0.81        34\n",
      "        True       0.00      0.00      0.00        16\n",
      "\n",
      "    accuracy                           0.68        50\n",
      "   macro avg       0.34      0.50      0.40        50\n",
      "weighted avg       0.46      0.68      0.55        50\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Afris\\miniconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Afris\\miniconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Afris\\miniconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "gen_report(model, \n",
    "[demo_test_df.iloc[:,:-2].values.astype(np.float32), test_images.astype(np.float32)],\n",
    "demo_test_df['label'].values,\n",
    "'Confusion Matrix - Statewide Model'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Oversampled Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "17/17 [==============================] - 86s 5s/step - loss: 0.9262 - accuracy: 0.5077 - val_loss: 0.6932 - val_accuracy: 0.5345\n",
      "Epoch 2/10\n",
      "17/17 [==============================] - 85s 5s/step - loss: 0.6931 - accuracy: 0.5019 - val_loss: 0.6932 - val_accuracy: 0.5172\n",
      "Epoch 3/10\n",
      "17/17 [==============================] - 86s 5s/step - loss: 0.6931 - accuracy: 0.4981 - val_loss: 0.6934 - val_accuracy: 0.5345\n",
      "Epoch 4/10\n",
      "17/17 [==============================] - 86s 5s/step - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6937 - val_accuracy: 0.5000\n",
      "Epoch 00004: early stopping\n"
     ]
    }
   ],
   "source": [
    "over_statewide_model = model.fit(\n",
    "\tx=[over_demo_train_df.iloc[:,:-2].values.astype(np.float32), over_train_images.astype(np.float32)], \n",
    "\ty=over_demo_train_df['label'].values,\n",
    "\tvalidation_data=([over_demo_val_df.iloc[:,:-2].values.astype(np.float32), over_val_images.astype(np.float32)], over_demo_val_df['label'].values),\n",
    "\tcallbacks=early,\n",
    "\tepochs=10,\n",
    "\tbatch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 35s 4s/step - loss: 0.6931 - accuracy: 0.5363\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6931073665618896, 0.5362903475761414]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate model\n",
    "over_eval_results = model.evaluate(x=[over_demo_test_df.iloc[:,:-2].values.astype(np.float32), over_test_images.astype(np.float32)],\n",
    "y=over_demo_test_df['label'].values)\n",
    "\n",
    "over_eval_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Only Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image preprocessing - separate generators in case I want to test images different from train/validation images\n",
    "data_generator = ImageDataGenerator(\n",
    "    #width_shift_range=0.1,\n",
    "    # shear_range=0.1,\n",
    "    zoom_range=0.3,\n",
    "    samplewise_center=True,\n",
    "    samplewise_std_normalization=True\n",
    ")\n",
    "\n",
    "test_generator = ImageDataGenerator(\n",
    "    # shear_range=0.1,\n",
    "    zoom_range=0.3,\n",
    "    samplewise_center=True,\n",
    "    samplewise_std_normalization=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 720 images belonging to 2 classes.\n",
      "Found 80 images belonging to 2 classes.\n",
      "Found 200 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# create train/test/val images with generators\n",
    "\n",
    "img_dir = 'Outputs/images/statewide/'\n",
    "\n",
    "data_train = data_generator.flow_from_directory(img_dir+'train/sub/', target_size=(224, 224), \n",
    "                            batch_size=32, class_mode='binary', \n",
    "                            shuffle=False,\n",
    "                            save_to_dir=img_dir+'train/processed/',\n",
    "                            # save_prefix='processed_'\n",
    "                            )\n",
    "data_val = data_generator.flow_from_directory(img_dir+'val/sub/', target_size=(224, 224), \n",
    "                            batch_size=32, class_mode='binary', \n",
    "                            shuffle=False,\n",
    "                            save_to_dir=img_dir+'val/processed/',\n",
    "                            # save_prefix='processed_'\n",
    "                            )\n",
    "\n",
    "data_test = test_generator.flow_from_directory(img_dir+'test/sub/', target_size=(224,224), \n",
    "                            batch_size=32, class_mode='binary',\n",
    "                            shuffle=False,\n",
    "                            save_to_dir=img_dir+'test/processed/',\n",
    "                            # save_prefix='processed_'\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 281 images belonging to 2 classes.\n",
      "Found 32 images belonging to 2 classes.\n",
      "Found 79 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# same thing for oversampled data\n",
    "\n",
    "over_train = data_generator.flow_from_directory(img_dir+'overs/train/', target_size=(224, 224), \n",
    "                            batch_size=16, class_mode='binary', \n",
    "                            # shuffle=True,\n",
    "                            # save_to_dir=img_dir+'processed/',\n",
    "                            # save_prefix='processed_'\n",
    "                            )\n",
    "over_val = data_generator.flow_from_directory(img_dir+'overs/val/', target_size=(224, 224), \n",
    "                            batch_size=16, class_mode='binary', \n",
    "                            # shuffle=True,\n",
    "                            # save_to_dir=img_dir+'processed/',\n",
    "                            # save_prefix='processed_'\n",
    "                            )\n",
    "\n",
    "over_test = test_generator.flow_from_directory(img_dir+'overs/test/', target_size=(224,224), \n",
    "                            batch_size=16, class_mode='binary',\n",
    "                            # shuffle=True,\n",
    "                            # save_to_dir=img_dir+'processed/',\n",
    "                            # save_prefix='processed_'\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build custom VGG16 CNN from scratch to allow for quick customization and tinkering\n",
    "\n",
    "cnn_model = Sequential()\n",
    "cnn_model.add(tf.keras.applications.VGG16(include_top=False, weights='imagenet', pooling='avg'))\n",
    "\n",
    "for layer in cnn_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "\n",
    "cnn_model.add(Flatten())\n",
    "\n",
    "cnn_model.add(Dense(4096, activation='relu'))\n",
    "\n",
    "cnn_model.add(Dense(4096, activation='relu'))\n",
    "\n",
    "cnn_model.add(Dense(1000, activation='relu'))\n",
    "\n",
    "cnn_model.add(Dense(500, activation='relu'))\n",
    "cnn_model.add(Dense(100, activation='relu'))\n",
    "\n",
    "cnn_model.add(Dense(10, activation='relu'))\n",
    "\n",
    "cnn_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "opt = Adam(learning_rate=1e-3, epsilon=1e-3 / 200)\n",
    "cnn_model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "12/12 [==============================] - 16s 1s/step - loss: 0.7229 - accuracy: 0.8222 - val_loss: 0.6996 - val_accuracy: 0.7000\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - 15s 1s/step - loss: 0.5011 - accuracy: 0.8222 - val_loss: 0.7203 - val_accuracy: 0.7000\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - 16s 1s/step - loss: 0.4955 - accuracy: 0.8222 - val_loss: 0.6196 - val_accuracy: 0.7000\n",
      "Epoch 4/20\n",
      "12/12 [==============================] - 15s 1s/step - loss: 0.4871 - accuracy: 0.8222 - val_loss: 0.7942 - val_accuracy: 0.7000\n",
      "Epoch 00004: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1977f0f6e20>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit CNN model\n",
    "cnn_model.fit(data_train, \n",
    "validation_data=data_val,\n",
    "callbacks=early,\n",
    "epochs=20,\n",
    "batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 4s 812ms/step - loss: 0.8091 - accuracy: 0.6800\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.8090676665306091, 0.6800000071525574]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate CNN model\n",
    "eval_results = cnn_model.evaluate(x=data_test, batch_size=16)\n",
    "\n",
    "eval_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Oversampled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "18/18 [==============================] - 24s 1s/step - loss: 0.7442 - accuracy: 0.5125 - val_loss: 0.7052 - val_accuracy: 0.3750\n",
      "Epoch 2/20\n",
      "18/18 [==============================] - 23s 1s/step - loss: 0.6929 - accuracy: 0.5018 - val_loss: 0.6935 - val_accuracy: 0.3750\n",
      "Epoch 3/20\n",
      "18/18 [==============================] - 22s 1s/step - loss: 0.6931 - accuracy: 0.5018 - val_loss: 0.6934 - val_accuracy: 0.3750\n",
      "Epoch 4/20\n",
      "18/18 [==============================] - 22s 1s/step - loss: 0.6932 - accuracy: 0.5018 - val_loss: 0.6934 - val_accuracy: 0.3750\n",
      "Epoch 00004: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1977f066d60>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_model.fit(over_train, \n",
    "validation_data=over_val,\n",
    "callbacks=early,\n",
    "epochs=20,\n",
    "batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 5s 1s/step - loss: 0.6931 - accuracy: 0.5443\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6930528283119202, 0.5443037748336792]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_results = cnn_model.evaluate(x=over_test, batch_size=16)\n",
    "\n",
    "eval_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Statewide Images\n",
    "    - Trying to see if tinkering with the statewide images will help the combined model like it did for the district models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the transformations on the images\n",
    "\n",
    "for i in range(len(data_val)):\n",
    "    data_val.next()\n",
    "\n",
    "for i in range(len(data_train)):\n",
    "    data_train.next()\n",
    "\n",
    "for i in range(len(data_test)):\n",
    "    data_test.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get original image file name\n",
    "val_images = [file.split('\\\\')[1] for file in data_val.filenames]\n",
    "test_images = [file.split('\\\\')[1] for file in data_test.filenames]\n",
    "train_images = [file.split('\\\\')[1] for file in data_train.filenames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wpop_diff</th>\n",
       "      <th>bpop_diff</th>\n",
       "      <th>area_diff</th>\n",
       "      <th>perim_diff</th>\n",
       "      <th>label</th>\n",
       "      <th>image_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>711</th>\n",
       "      <td>0.652815</td>\n",
       "      <td>0.611372</td>\n",
       "      <td>0.551100</td>\n",
       "      <td>0.396517</td>\n",
       "      <td>False</td>\n",
       "      <td>state_plot7120.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.283966</td>\n",
       "      <td>0.323942</td>\n",
       "      <td>0.554893</td>\n",
       "      <td>0.539535</td>\n",
       "      <td>False</td>\n",
       "      <td>state_plot990.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>937</th>\n",
       "      <td>0.418857</td>\n",
       "      <td>0.325763</td>\n",
       "      <td>0.623737</td>\n",
       "      <td>0.436361</td>\n",
       "      <td>True</td>\n",
       "      <td>state_plot9380.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357</th>\n",
       "      <td>0.505302</td>\n",
       "      <td>0.466818</td>\n",
       "      <td>0.725118</td>\n",
       "      <td>0.359819</td>\n",
       "      <td>False</td>\n",
       "      <td>state_plot3580.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>701</th>\n",
       "      <td>0.279588</td>\n",
       "      <td>0.372019</td>\n",
       "      <td>0.841260</td>\n",
       "      <td>0.178652</td>\n",
       "      <td>True</td>\n",
       "      <td>state_plot7020.png</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     wpop_diff  bpop_diff  area_diff  perim_diff  label          image_file\n",
       "711   0.652815   0.611372   0.551100    0.396517  False  state_plot7120.png\n",
       "98    0.283966   0.323942   0.554893    0.539535  False   state_plot990.png\n",
       "937   0.418857   0.325763   0.623737    0.436361   True  state_plot9380.png\n",
       "357   0.505302   0.466818   0.725118    0.359819  False  state_plot3580.png\n",
       "701   0.279588   0.372019   0.841260    0.178652   True  state_plot7020.png"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wpop_diff</th>\n",
       "      <th>bpop_diff</th>\n",
       "      <th>area_diff</th>\n",
       "      <th>perim_diff</th>\n",
       "      <th>label</th>\n",
       "      <th>image_file</th>\n",
       "      <th>process_image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>711</th>\n",
       "      <td>0.652815</td>\n",
       "      <td>0.611372</td>\n",
       "      <td>0.551100</td>\n",
       "      <td>0.396517</td>\n",
       "      <td>False</td>\n",
       "      <td>state_plot7120.png</td>\n",
       "      <td>_0_9082966.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.283966</td>\n",
       "      <td>0.323942</td>\n",
       "      <td>0.554893</td>\n",
       "      <td>0.539535</td>\n",
       "      <td>False</td>\n",
       "      <td>state_plot990.png</td>\n",
       "      <td>_1_5490020.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>937</th>\n",
       "      <td>0.418857</td>\n",
       "      <td>0.325763</td>\n",
       "      <td>0.623737</td>\n",
       "      <td>0.436361</td>\n",
       "      <td>True</td>\n",
       "      <td>state_plot9380.png</td>\n",
       "      <td>_2_6799282.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357</th>\n",
       "      <td>0.505302</td>\n",
       "      <td>0.466818</td>\n",
       "      <td>0.725118</td>\n",
       "      <td>0.359819</td>\n",
       "      <td>False</td>\n",
       "      <td>state_plot3580.png</td>\n",
       "      <td>_3_2880876.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>701</th>\n",
       "      <td>0.279588</td>\n",
       "      <td>0.372019</td>\n",
       "      <td>0.841260</td>\n",
       "      <td>0.178652</td>\n",
       "      <td>True</td>\n",
       "      <td>state_plot7020.png</td>\n",
       "      <td>_4_7040159.png</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     wpop_diff  bpop_diff  area_diff  perim_diff  label          image_file  \\\n",
       "711   0.652815   0.611372   0.551100    0.396517  False  state_plot7120.png   \n",
       "98    0.283966   0.323942   0.554893    0.539535  False   state_plot990.png   \n",
       "937   0.418857   0.325763   0.623737    0.436361   True  state_plot9380.png   \n",
       "357   0.505302   0.466818   0.725118    0.359819  False  state_plot3580.png   \n",
       "701   0.279588   0.372019   0.841260    0.178652   True  state_plot7020.png   \n",
       "\n",
       "      process_image  \n",
       "711  _0_9082966.png  \n",
       "98   _1_5490020.png  \n",
       "937  _2_6799282.png  \n",
       "357  _3_2880876.png  \n",
       "701  _4_7040159.png  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add column to dataframes for new processed image file names\n",
    "demo_test_df['process_image'] = sorted(os.listdir(path=img_dir+'test/processed'), key=lambda x: int(str(x)[1:].split('_')[0]))\n",
    "demo_train_df['process_image'] = sorted(os.listdir(path=img_dir+'train/processed'), key=lambda x: int(str(x)[1:].split('_')[0]))\n",
    "demo_val_df['process_image'] = sorted(os.listdir(path=img_dir+'val/processed'), key=lambda x: int(str(x)[1:].split('_')[0]))\n",
    "\n",
    "demo_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert processed images to arrays\n",
    "train_images_pross = img_array(demo_train_df['process_image'], subset='train',resize=True)\n",
    "test_images_pross = img_array(demo_test_df['process_image'], subset='test',resize=True)\n",
    "val_images_pross = img_array(demo_val_df['process_image'], subset='val',resize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train, Test, and Fit Model with Preprocessed Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "23/23 [==============================] - 52s 2s/step - loss: 0.7462 - accuracy: 0.3667 - val_loss: 0.7160 - val_accuracy: 0.4000\n",
      "Epoch 2/20\n",
      "23/23 [==============================] - 51s 2s/step - loss: 0.7119 - accuracy: 0.3681 - val_loss: 0.7050 - val_accuracy: 0.3875\n",
      "Epoch 3/20\n",
      "23/23 [==============================] - 53s 2s/step - loss: 0.6995 - accuracy: 0.3958 - val_loss: 0.6964 - val_accuracy: 0.4875\n",
      "Epoch 4/20\n",
      "23/23 [==============================] - 50s 2s/step - loss: 0.6903 - accuracy: 0.5778 - val_loss: 0.6896 - val_accuracy: 0.5750\n",
      "Epoch 5/20\n",
      "23/23 [==============================] - 50s 2s/step - loss: 0.6832 - accuracy: 0.6250 - val_loss: 0.6855 - val_accuracy: 0.6000\n",
      "Epoch 6/20\n",
      "23/23 [==============================] - 47s 2s/step - loss: 0.6776 - accuracy: 0.6333 - val_loss: 0.6811 - val_accuracy: 0.6000\n",
      "Epoch 7/20\n",
      "23/23 [==============================] - 47s 2s/step - loss: 0.6707 - accuracy: 0.6333 - val_loss: 0.6772 - val_accuracy: 0.6000\n",
      "Epoch 8/20\n",
      "23/23 [==============================] - 46s 2s/step - loss: 0.6650 - accuracy: 0.6333 - val_loss: 0.6752 - val_accuracy: 0.6000\n",
      "Epoch 00008: early stopping\n"
     ]
    }
   ],
   "source": [
    "pross_statewide_model = model.fit(\n",
    "\tx=[demo_train_df.iloc[:,:-3].values.astype(np.float32), train_images_pross.astype(np.float32)], \n",
    "\ty=demo_train_df['label'].values,\n",
    "\tvalidation_data=([demo_val_df.iloc[:,:-3].values.astype(np.float32), val_images_pross.astype(np.float32)], demo_val_df['label'].values),\n",
    "\tcallbacks=early,\n",
    "\tepochs=20,\n",
    "\tbatch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 11s 2s/step - loss: 0.6816 - accuracy: 0.5800\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6816393136978149, 0.5799999833106995]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate model\n",
    "eval_results = model.evaluate(x=[demo_test_df.iloc[:,:-3].values.astype(np.float32), test_images_pross.astype(np.float32)],\n",
    "y=demo_test_df['label'].values)\n",
    "\n",
    "eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.58      1.00      0.73       116\n",
      "        True       0.00      0.00      0.00        84\n",
      "\n",
      "    accuracy                           0.58       200\n",
      "   macro avg       0.29      0.50      0.37       200\n",
      "weighted avg       0.34      0.58      0.43       200\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Afris\\miniconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Afris\\miniconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Afris\\miniconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "gen_report(model, \n",
    "[demo_test_df.iloc[:,:-3].values.astype(np.float32), test_images_pross.astype(np.float32)],\n",
    "demo_test_df['label'].values,\n",
    "'Confusion Matrix - Statewide Model'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same results!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "992af9430b13ada16c30f2ac47cf0d744148870fc875adc12e54130686e91185"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
