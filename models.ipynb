{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.layers import Dense\n",
    "from keras import Sequential\n",
    "from tensorflow.keras.models import Model\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPool2D\n",
    "from keras.layers import ReLU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import keras_tuner as kt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_vgg16(dropout=None, train_layers=False, output_layer=False):\n",
    "    \"\"\"\n",
    "    Creates a VGG16 CNN with custom top layers.\n",
    "\n",
    "    Paramters: \n",
    "        dropout: float - dropout rate for all dropout layers. Must be between 0 and 1, default None (no dropout layers)\n",
    "        train_layers: Boolean - whether or not to train the layers within the VGG16 network. Default False\n",
    "        output_layer: Boolean - inclusion of final output layer with a single node and sigmoid activation. Default False\n",
    "\n",
    "    return: \n",
    "        Keras model - VGG16 Keras model with custom top layers \n",
    "    \"\"\"\n",
    "    # VGG16 network\n",
    "    base_model = tf.keras.applications.VGG16(include_top=False, weights='imagenet', pooling='avg')\n",
    "\n",
    "    # optional training of VGG16 layers\n",
    "    if not train_layers:\n",
    "        for layer in base_model.layers:\n",
    "            layer.trainable = False\n",
    "    else:\n",
    "        for layer in base_model.layers:\n",
    "            layer.trainable = True\n",
    "\n",
    "    # flatten final VGG16 layer\n",
    "    x = base_model.output\n",
    "    x = Flatten()(x)\n",
    "    \n",
    "    # generate fully-connected layers with optional dropout layers\n",
    "    x = Dense(4096, activation='relu')(x)\n",
    "    if dropout is not None:\n",
    "        x = Dropout(dropout)(x)\n",
    "    x = Dense(4096, activation='relu')(x)\n",
    "    if dropout is not None:\n",
    "        x = Dropout(dropout)(x)\n",
    "    x = Dense(1000, activation='relu')(x)\n",
    "    if dropout is not None:\n",
    "        x = Dropout(dropout)(x)\n",
    "    x = Dense(500, activation='relu')(x)\n",
    "    if dropout is not None:\n",
    "        x = Dropout(dropout)(x)\n",
    "    x = Dense(100, activation='relu')(x)\n",
    "    if dropout is not None:\n",
    "        x = Dropout(dropout)(x)\n",
    "\n",
    "    \n",
    "    x = Dense(10, activation='relu')(x)\n",
    "\n",
    "    # final layer\n",
    "    x = Dense(4, activation='relu')(x)\n",
    "\n",
    "    # optional output layer if creating solo model\n",
    "    if output_layer:\n",
    "        x = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(inputs=base_model.input, outputs=x)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple NN\n",
    "def make_nn(input_nodes, output_layer=False):\n",
    "    \"\"\"\n",
    "    Creates a basic neural network model with 1 hidden layer.\n",
    "\n",
    "    Paramters: \n",
    "        input_nodes: int - number of input nodes/dimensions/features \n",
    "        output_layer: Boolean - inclusion of final output layer with 2 nodes and softmax activation. Default False\n",
    "\n",
    "    return: \n",
    "        Keras model - neural network Keras model\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Dense(input_nodes, input_dim=input_nodes, activation='relu'))\n",
    "\n",
    "    model.add(Dense(input_nodes, activation='relu'))\n",
    "\n",
    "    if output_layer:\n",
    "        model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kt_model_build(hp):\n",
    "    \"\"\"\n",
    "    Creates Keras-Tuner model for parameter searching/tuning. This function has the largest set of search parameters.\n",
    "\n",
    "    Paramters: \n",
    "        None\n",
    "\n",
    "    return: \n",
    "        Keras model - combined VGG16 and neural network model with selected search parameters \n",
    "    \"\"\"\n",
    "    # build basic NN first\n",
    "    nn_model = Sequential()\n",
    "\n",
    "    # activation functions to select from\n",
    "    activation_list = ['relu', 'linear', 'elu', 'gelu', 'selu']\n",
    "\n",
    "    # input nodes have to be same as total features, i.e., 4\n",
    "    nn_input_nodes = 4\n",
    "\n",
    "    # select NN input layer activation function\n",
    "    input_activation = hp.Choice('input_activation', activation_list)\n",
    "    \n",
    "    # input NN layer\n",
    "    nn_model.add(Dense(nn_input_nodes, input_dim=nn_input_nodes, activation=input_activation))\n",
    "\n",
    "    # generate hidden layers\n",
    "    for i in range(hp.Int('num_layers', 1, 5)):\n",
    "\n",
    "        layer_activation = hp.Choice(f'activation_{i}', activation_list)\n",
    "\n",
    "        nn_model.add(Dense(units=hp.Int(f'nodes_{i}', min_value=3, max_value=36, step=1),\n",
    "                        activation=layer_activation\n",
    "        ))\n",
    "    \n",
    "    # final NN layer\n",
    "    nn_model.add(Dense(4, activation='relu'))\n",
    "\n",
    "\n",
    "\n",
    "    # build VGG16/CNN\n",
    "    vgg_model = tf.keras.applications.VGG16(include_top=False, weights='imagenet', pooling='avg')\n",
    "\n",
    "    # don't train VGG16 layers\n",
    "    for layer in vgg_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    x = vgg_model.output\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(4096, activation='relu')(x)\n",
    "\n",
    "    # first set of layers with nodes between 1000-5000\n",
    "    for i in range(hp.Int('cnn_set1_num_layers', 1, 3)):\n",
    "        layer_activation = hp.Choice(f'cnn_set1_activation_{i}', activation_list)\n",
    "        layer_nodes = hp.Int(f'cnn_set1_nodes_{i}', min_value=1000, max_value=5000, step=100)\n",
    "\n",
    "        x = Dense(layer_nodes, activation=layer_activation)(x)\n",
    "    \n",
    "    # second set of layers with nodes between 10-500\n",
    "    for i in range(hp.Int('cnn_set2_num_layers', 1, 3)):\n",
    "        layer_activation = hp.Choice(f'cnn_set2_activation_{i}', activation_list)\n",
    "        layer_nodes = hp.Int(f'cnn_set2_nodes_{i}', min_value=10, max_value=500, step=10)\n",
    "\n",
    "        x = Dense(layer_nodes, activation=layer_activation)(x)\n",
    "\n",
    "\n",
    "    # final CNN layer\n",
    "    x = Dense(4, activation='relu')(x)\n",
    "\n",
    "    cnn_model = Model(inputs=vgg_model.input, outputs=x)\n",
    "\n",
    "    # combine models\n",
    "    combined_model = tf.keras.layers.concatenate([nn_model.output, cnn_model.output], axis=1)\n",
    "    combined_model = Dense(4, activation='relu')(combined_model)\n",
    "\n",
    "    # final combined output layer\n",
    "    combined_model = Dense(1, activation='sigmoid')(combined_model)\n",
    "\n",
    "    final_model = Model(inputs=[nn_model.input, cnn_model.input], outputs=combined_model)\n",
    "\n",
    "    # compile model\n",
    "    # sample values for learning rate and epsilon\n",
    "    lr = hp.Float(\"learning_rate\", min_value=1e-5, max_value=1e-1, sampling=\"log\")\n",
    "    ep = hp.Float('epsilon', min_value=1e-9, max_value=1e-5, sampling='log')\n",
    "\n",
    "    # using Adam optimizer\n",
    "    opt = Adam(learning_rate=lr, epsilon=ep)\n",
    "\n",
    "    # compile model\n",
    "    final_model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "    return final_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trimmed_kt_model_build(hp):\n",
    "    \"\"\"\n",
    "    Creates Keras-Tuner model for parameter searching/tuning. This function has a trimmed set of search parameters.\n",
    "\n",
    "    Paramters: \n",
    "        None\n",
    "\n",
    "    return: \n",
    "        Keras model - combined VGG16 and neural network model with selected search parameters \n",
    "    \"\"\"\n",
    "\n",
    "    # build basic NN first - same general architecture as kt_model_build function\n",
    "    nn_model = Sequential()\n",
    "\n",
    "    # activation functions to select from\n",
    "    activation_list = ['relu', 'gelu', 'selu']\n",
    "\n",
    "    # input nodes have to be same # as total features, i.e., 4\n",
    "    nn_input_nodes = 4\n",
    "\n",
    "    input_activation = hp.Choice('input_activation', activation_list)\n",
    "    nn_model.add(Dense(nn_input_nodes, input_dim=nn_input_nodes, activation=input_activation))\n",
    "\n",
    "    # generate hidden layers\n",
    "    for i in range(hp.Int('num_layers', 3, 5)):\n",
    "\n",
    "        layer_activation = hp.Choice(f'activation_{i}', activation_list)\n",
    "\n",
    "        nn_model.add(Dense(units=hp.Int(f'nodes_{i}', min_value=3, max_value=36, step=3),\n",
    "                        activation=layer_activation\n",
    "        ))\n",
    "    \n",
    "    final_nodes = 4\n",
    "    nn_model.add(Dense(final_nodes, activation='relu'))\n",
    "\n",
    "\n",
    "    # build CNN\n",
    "    vgg_model = tf.keras.applications.VGG16(include_top=False, weights='imagenet', pooling='avg')\n",
    "\n",
    "    for layer in vgg_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    x = vgg_model.output\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(4096, activation='relu')(x)\n",
    "\n",
    "    # sample dropout rate for dropout layers, values between 0.15-0.35\n",
    "    droprate = hp.Float('droprate', min_value=0.15, max_value=0.35, step=0.05)\n",
    "    x = Dropout(droprate)(x)\n",
    "\n",
    "    # set of layers with nodes between 1000-4000, dropout layers after each \n",
    "    for i in range(hp.Int('cnn_set1_num_layers', 3, 6)):\n",
    "        layer_activation = hp.Choice(f'cnn_set1_activation_{i}', activation_list)\n",
    "        layer_nodes = hp.Int(f'cnn_set1_nodes_{i}', min_value=1000, max_value=4000, step=500)\n",
    "\n",
    "        x = Dense(layer_nodes, activation=layer_activation)(x)\n",
    "        x = Dropout(droprate)(x)\n",
    "\n",
    "    # final Dense layers\n",
    "    x = Dense(500, activation='relu')(x)\n",
    "    x = Dropout(droprate)(x)\n",
    "    x = Dense(100, activation='relu')(x)\n",
    "    x = Dropout(droprate)(x)\n",
    "\n",
    "    x = Dense(final_nodes, activation='relu')(x) \n",
    "\n",
    "    cnn_model = Model(inputs=vgg_model.input, outputs=x)\n",
    "\n",
    "    # combine models\n",
    "    combined_model = tf.keras.layers.concatenate([nn_model.output, cnn_model.output],axis=1)\n",
    "    combined_model = Dense(final_nodes, activation='relu')(combined_model)\n",
    "\n",
    "    # final output layer\n",
    "    combined_model = Dense(1, activation='sigmoid')(combined_model)\n",
    "\n",
    "    final_model = Model(inputs=[nn_model.input, cnn_model.input], outputs=combined_model)\n",
    "\n",
    "    # sample learning rate and epsilon\n",
    "    lr = hp.Float(\"learning_rate\", min_value=1e-4, max_value=1e-1, sampling=\"log\")\n",
    "    ep = hp.Float('epsilon', min_value=1e-5, max_value=1e-3, sampling='log')\n",
    "\n",
    "    opt = Adam(learning_rate=lr, epsilon=ep)\n",
    "\n",
    "    # compile model\n",
    "    final_model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "    return final_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cat_kt_model_build(hp):\n",
    "    \"\"\"\n",
    "    Creates Keras-Tuner model for parameter searching/tuning for a multiclass model (low, medium, high output layer). \n",
    "    This function has a trimmed set of search parameters.\n",
    "\n",
    "    Paramters: \n",
    "        None\n",
    "\n",
    "    return: \n",
    "        Keras model - combined VGG16 and neural network model with selected search parameters \n",
    "    \"\"\"\n",
    "\n",
    "    # build basic NN first - same general architecture as kt_model_build function except different output layer\n",
    "    nn_model = Sequential()\n",
    "\n",
    "    activation_list = ['relu', 'gelu', 'selu']\n",
    "    # input nodes have to be same # as total features, i.e., 4\n",
    "    nn_input_nodes = 4\n",
    "\n",
    "    input_activation = hp.Choice('input_activation', activation_list)\n",
    "    nn_model.add(Dense(nn_input_nodes, input_dim=nn_input_nodes, activation=input_activation))\n",
    "\n",
    "    # generate hidden layers with nodes between 3-36 \n",
    "    for i in range(hp.Int('num_layers', 3, 5)):\n",
    "\n",
    "        layer_activation = hp.Choice(f'activation_{i}', activation_list)\n",
    "        nn_model.add(Dense(units=hp.Int(f'nodes_{i}', min_value=3, max_value=36, step=3),\n",
    "                        activation=layer_activation\n",
    "        ))\n",
    "    \n",
    "    # final NN layer\n",
    "    nn_model.add(Dense(3, activation='relu'))\n",
    "\n",
    "\n",
    "    # build CNN\n",
    "    vgg_model = tf.keras.applications.VGG16(include_top=False, weights='imagenet', pooling='avg')\n",
    "\n",
    "    for layer in vgg_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    x = vgg_model.output\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(4096, activation='relu')(x)\n",
    "\n",
    "    # set dropout rate for dropout layers\n",
    "    droprate = hp.Float('droprate', min_value=0.15, max_value=0.35, step=0.05)\n",
    "    x = Dropout(droprate)(x)\n",
    "\n",
    "    # generate dense layers with nodes between 1000-4000, dropout layers after each\n",
    "    for i in range(hp.Int('cnn_set1_num_layers', 3, 6)):\n",
    "        layer_activation = hp.Choice(f'cnn_set1_activation_{i}', activation_list)\n",
    "        layer_nodes = hp.Int(f'cnn_set1_nodes_{i}', min_value=1000, max_value=4000, step=500)\n",
    "\n",
    "        x = Dense(layer_nodes, activation=layer_activation)(x)\n",
    "        x = Dropout(droprate)(x)\n",
    "\n",
    "    # final dense layers\n",
    "    x = Dense(500, activation='relu')(x)\n",
    "    x = Dropout(droprate)(x)\n",
    "    x = Dense(100, activation='relu')(x)\n",
    "    x = Dropout(droprate)(x)\n",
    "\n",
    "    # final CNN layer\n",
    "    x = Dense(3, activation='relu')(x)\n",
    "\n",
    "    cnn_model = Model(inputs=vgg_model.input, outputs=x)\n",
    "\n",
    "    # combine models\n",
    "    combined_model = tf.keras.layers.concatenate([nn_model.output, cnn_model.output],axis=1)\n",
    "    combined_model = Dense(3, activation='relu')(combined_model)\n",
    "\n",
    "    # final output layer - 3 nodes, softmax activation\n",
    "    combined_model = Dense(3, activation='softmax')(combined_model)\n",
    "\n",
    "    final_model = Model(inputs=[nn_model.input, cnn_model.input], outputs=combined_model)\n",
    "\n",
    "    # sample learning rate and epsilon\n",
    "    lr = hp.Float(\"learning_rate\", min_value=1e-4, max_value=1e-1, sampling=\"log\")\n",
    "    ep = hp.Float('epsilon', min_value=1e-7, max_value=1e-4, sampling='log')\n",
    "\n",
    "    opt = Adam(learning_rate=lr, epsilon=ep)\n",
    "\n",
    "    # compile model - set loss to 'sparse_categorical_crossentropy' for multiclass\n",
    "    final_model.compile(loss='sparse_categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "    return final_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extra_trimmed_kt_model_build(hp):\n",
    "    \"\"\"\n",
    "    Creates Keras-Tuner model for parameter searching/tuning. This function has the most limited set of search parameters.\n",
    "\n",
    "    Paramters: \n",
    "        None\n",
    "\n",
    "    return: \n",
    "        Keras model - combined VGG16 and neural network model with selected search parameters \n",
    "    \"\"\"\n",
    "\n",
    "    # build basic NN first - same general architecture as kt_model_build function\n",
    "    nn_model = Sequential()\n",
    "\n",
    "    activation_list = ['relu', 'gelu', 'selu']\n",
    "\n",
    "    # input nodes have to be same # as total features, i.e., 4\n",
    "    nn_input_nodes = 4\n",
    "\n",
    "    # activation function to use throughout model\n",
    "    input_activation = hp.Choice('input_activation', activation_list)\n",
    "\n",
    "    nn_model.add(Dense(nn_input_nodes, input_dim=nn_input_nodes, activation=input_activation))\n",
    "\n",
    "    # add 3 hidden layers with nodes between 3-36\n",
    "    nn_model.add(Dense(units=hp.Int(f'nodes_1', min_value=3, max_value=36, step=3),\n",
    "                    activation=input_activation\n",
    "    ))\n",
    "    nn_model.add(Dense(units=hp.Int(f'nodes_2', min_value=3, max_value=36, step=3),\n",
    "                    activation=input_activation\n",
    "    ))\n",
    "    nn_model.add(Dense(units=hp.Int(f'nodes_3', min_value=3, max_value=36, step=3),\n",
    "                    activation=input_activation\n",
    "    ))\n",
    "    \n",
    "    # final NN layer\n",
    "    final_nodes = 4\n",
    "    nn_model.add(Dense(final_nodes, activation='relu'))\n",
    "\n",
    "\n",
    "    # build CNN\n",
    "    vgg_model = tf.keras.applications.VGG16(include_top=False, weights='imagenet', pooling='avg')\n",
    "\n",
    "    for layer in vgg_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    x = vgg_model.output\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(4096, activation='relu')(x)\n",
    "\n",
    "    # set dropout rate between 0.15-0.35\n",
    "    droprate = hp.Float('droprate', min_value=0.15, max_value=0.35, step=0.05)\n",
    "    x = Dropout(droprate)(x)\n",
    "\n",
    "    # create 3 layers with nodes between 1000-4000 and dropout layers between them\n",
    "    layer1_nodes = hp.Int(f'cnn_nodes_1', min_value=1000, max_value=4000, step=500)\n",
    "\n",
    "    x = Dense(layer1_nodes, activation=input_activation)(x)\n",
    "    x = Dropout(droprate)(x)\n",
    "\n",
    "    layer2_nodes = hp.Int(f'cnn_nodes_2', min_value=1000, max_value=4000, step=500)\n",
    "    x = Dense(layer2_nodes, activation=input_activation)(x)\n",
    "    x = Dropout(droprate)(x)\n",
    "\n",
    "    layer3_nodes = hp.Int(f'cnn_nodes_3', min_value=1000, max_value=4000, step=500)\n",
    "    x = Dense(layer3_nodes, activation=input_activation)(x)\n",
    "    x = Dropout(droprate)(x)\n",
    "\n",
    "    # final CNN dense/dropout layers\n",
    "    x = Dense(500, activation=input_activation)(x)\n",
    "    x = Dropout(droprate)(x)\n",
    "    x = Dense(100, activation='relu')(x)\n",
    "    x = Dropout(droprate)(x)\n",
    "\n",
    "    # final CNN layer\n",
    "    x = Dense(final_nodes, activation='relu')(x)\n",
    "\n",
    "    cnn_model = Model(inputs=vgg_model.input, outputs=x)\n",
    "\n",
    "    # combine models\n",
    "    combined_model = tf.keras.layers.concatenate([nn_model.output, cnn_model.output],axis=1)\n",
    "    combined_model = Dense(final_nodes, activation='relu')(combined_model)\n",
    "\n",
    "    # final output layer\n",
    "    combined_model = Dense(1, activation='sigmoid')(combined_model)\n",
    "\n",
    "    final_model = Model(inputs=[nn_model.input, cnn_model.input], outputs=combined_model)\n",
    "\n",
    "    # sample learning rate and epsilon values\n",
    "    lr = hp.Float(\"learning_rate\", min_value=1e-4, max_value=1e-1, sampling=\"log\")\n",
    "    ep = hp.Float('epsilon', min_value=1e-5, max_value=1e-3, sampling='log')\n",
    "\n",
    "    opt = Adam(learning_rate=lr, epsilon=ep)\n",
    "\n",
    "    # compile model\n",
    "    final_model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "    return final_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "992af9430b13ada16c30f2ac47cf0d744148870fc875adc12e54130686e91185"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
